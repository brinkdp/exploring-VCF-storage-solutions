{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac14f0c0-ce9e-4cf1-9fac-400ccd9fe220",
   "metadata": {},
   "source": [
    "# Exploring VCF Storage Solutions\n",
    "# Notebook 1. Introduction to the project and preparation of example data\n",
    "2025-02-12 Daniel P. Brink\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Background: \n",
    "\n",
    "The Jupyter notebooks collected in this in this GitHub repository - [titled Exploring VCF Storage Solutions](https://github.com/brinkdp/exploring-zarr-for-VCFs/tree/main/notebooks) -  are indended to document an investigation into a handful of software solutions designed to improve various aspects of handling genetic variant data stored in the VCF (Variant Call Format). The phrase _VCF Storage_ in the title is intended as a wider concept that just data storage: a common denominator among the tools that are investigated here are that VCF files are ingested in a new data storage format, queried and transformed with commands within and/or adjacent to the tools, saved to new version that will need to be tracked with metadata, and then possibly exported back to VCF in case compatibility with downstream bioinformatic pipelines is needed. Thus, these tools are not database management systems in the traditional sense, nor are they stricly computational tools. The phrase _VCF Storage_ might fail to communicate the width of use-cases that these tools could encompass, but it will do for now _in lieu_ of better descriptor.\n",
    "\n",
    "The VCF format originated in during the [1000 Genomes project](https://www.internationalgenome.org/1000-genomes-summary) along with other seminal bioinformatics tools, utilities, and formats like SAMtools, HTSlib, and the SAM/BAM formats. The VCF format is a row-oriented tabular text file. While this makes VCFs quite human-readable and fairly easy to process with custom scripts, it is considered to be inefficient in terms of performance and storage ([Danechek et al., 2021](https://academic.oup.com/gigascience/article/10/2/giab008/6137722)). To try to improve processing times, a binary VCF version called BCF (binary VCF) has been developed. The specification for the VCF and BCF formats [can be found here](https://samtools.github.io/hts-specs/VCFv4.5.pdf) (v4.5, the latest version at the time of writing). \n",
    "\n",
    "Today, the gold standard tool for working with genetic variant data is `bcftools` ([see the manual here](https://samtools.github.io/bcftools/bcftools.html)). It supports a plethora of operations on data stored in compressed and uncompressed versions of the VCF and the BCF formats, from variant calling to data analysis operations. It is considered to be a highly optimized tool as far as VCF handling goes; in fact, it is probably more than sufficient for most users, although processing times can be quite substantial for large multi-sample datasets. \n",
    "\n",
    "So why investigate alternative tools at all, if `bcftools` is so efficient? The short answer is not VCF is not well suited at scale. Technological improvements in nucleotide sequencing - such as increased throughput, data quality, and read length - have been substantial since the first developments of the VCF format in the late 2000s, and data is, as a result, being generated at a higher rate and volume than before. Consequently, genetic variant data will also scale, adding more and more sequenced individuals to the populations described by the VCF files. It is thus likely that at some point in time - in the close future?, in the a distant future?, or, perhaps, in some cases already today? - the current way of working with the VCF format will pose a bottleneck in scientific analysis.\n",
    "\n",
    "It should thus not come as a surprise that there are people that are working on adressing this by developing new technological solutions. As of the time of writing there are already a few available tools that claim to adress this problem. The idea behind this collection of notebooks is to investigate how they work, their benefits, drawbacks, and limitations. The focus will be on open-source solutions that can be tested without the need to pay for a licence. However, some of the tools that will be investigated are built around hybrid models where the core functionalities are available in open source releases and advanced features are offered for paying customers. Any new solution for handling VCF files will inevitably be compared to the standard operations of `bcftools`. Ideally, the candidate tools should be able to perform the same operations, and preferrably do so more efficiently.\n",
    "\n",
    "\"But what do you mean with ´more effciently´?\", you might ask. In general, this investigation will consider efficiency in terms of three key paramaters:\n",
    "\n",
    "- Processing performance\n",
    "    - How long time and how much compute resources is required to perform standard VCF operations?\n",
    "- Storage performance\n",
    "    - How much disk space is required for storing the data. Is there efficient compression in place? How does the I/O of the software perform in a cloud storage enviroment.\n",
    "- Data management\n",
    "    - How can the software help the user to keep track of metadata and versioning?\n",
    "\n",
    "The tools that will be investigated are:\n",
    "\n",
    "- VCF Zarr\n",
    "- TileDB-VCF\n",
    "- OpenGCA\n",
    "\n",
    "(more tools might be considered at a later stage.)\n",
    "\n",
    "## 1.2. Outline of the notebooks\n",
    "\n",
    "While these notebooks are essentially a vector to document a technical assessment of these tools, they have been writen in the ethos of Open Science and reproducible research. They serve both to document the findings of the investigations but also capture the process of work itself. The latter essentially leads to a much more verbose documentation that might be less efficient for readers that quickly want to find a particular piece of information, but hopefully valuable for readers intrested in reproducibility and learning about the tools. A \"semi-naïve\" approach was used for the tests cover the notebooks: with this I mean that part of the investigation was peformed by going in blind and discovering features, quirks, and errors organically, while other parts were based on reading documentation and tutorials before doing the tests. By working in this manner, the idea was to avoid reproducing existing documentation that the respective developers already provide and maintain, and instead focus on the experiences gained from interacting with the documentation and the tools.\n",
    "\n",
    "One way to think about the individual notebooks is as chapters in a book. A rough table-of-contents for the planned notebooks looks something like this:\n",
    "\n",
    "- Notebook 1. Introduction to the project and preparation of example data\n",
    "- Notebook 2. Overview of commonly used bcftools operations (point-of-reference for all the comparisons)\n",
    "- Notebook 3.1. VCF Zarr Part I\n",
    "- Notebook 3.2. VCF Zarr Part II\n",
    "- Notebook 4.1. TileDB-VCF Part I\n",
    "- Notebook 4.1. TileDB-VCF Part II\n",
    "- Notebook 5.1. OpenGCA Part I\n",
    "- Notebook 5.2. OpenGCA Part II\n",
    "\n",
    "The first two notebooks set the scope and serve as point-of-reference. New readers are reccomended to start there. Through the use of conda enviroments, the intention is that reader should be able to re-run the code on their own machines, reproducing the results and output files. Section 2 in each notebook will specify the installations commands that were used to setup each environment. Since the evaluated software have different dependencies, we will in general make one enviroment per evaluated tool.\n",
    "\n",
    "The notebooks for the tools use subnumbering to be able to split the content into smaller chunks and to allow to add more notebooks for a tools over time without having to retroactively have to change the numbering. Notebooks that start with 3 are about VCF Zarr, 4 are about TileDB-VCF, and 5 are about OpenGCA. The notebooks covering the tools are meant to be read independently of each other, but their subnumbered notebooks are written to be read in order, starting with notebook X.1 (e.g. 3.1 -> 3.2 -> 3.3. etc.). In general, the order of the tests are preserved across the notebooks, meaning that a test performed for VCF Zarr in 3.1 is likely to have its TileDB-VCF equivalent reported in 4.1., and so on.\n",
    "\n",
    "## 1.3. Preparing an example dataset for use across all notebooks\n",
    "In order to make the exploration of these tools as systematic and comparable as possible, we will first start by preparing an example small dataset that will be used for testing all software. We will be using public human VCF data from the 1000 Genomes project (Section 3 in the current notebook). This is considered a large dataset, encompassing millions of variants across ~2500 samples. Using this data could eventually help us evaluate the performance of the software candidates at scale, should we need and desire to do so. However, for the intial operations, this data will take unneccessarily long to process, so we will downsample it in a reproducible manner to generate a small-to-medium sized dataset for testing out the basic operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3a46f-52a5-47a0-88b1-0a1581c0cee2",
   "metadata": {},
   "source": [
    "# 2. Setup\n",
    "\n",
    "All investigations in these notebooks will be done on a Mac M3 laptop. Conda environments will be used for reproducibility. Since there might be conflicting dependencies between the different tools that will be explored in these notebooks, we might end up using different environments for each notebook. The environments will be specified in the start of each notebook, as below. For the first two notebooks we will basically just need python and `bcftools` so the environment will be very slim.\n",
    "\n",
    "```bash\n",
    "CONDA_SUBDIR=osx-64 conda create -n explore_vcf_storage_solutions\n",
    "conda activate explore_vcf_storage_solutions\n",
    "conda install mamba -y\n",
    "mamba install jupyter -y\n",
    "mamba install bcftools -y \n",
    "pip install humanfriendly\n",
    "```\n",
    "\n",
    "\n",
    "`CONDA_SUBDIR=osx-64 conda` is used to ensure compatibility with the CPU architechture of the M series Macs. It essentially emulates an AMD64 chipset. Like all emulation, it comes at a cost of processing overhead, but for the needs of these notebooks we can live with that. But keep in mind that these notebooks are not focusing on optimizing the performance.\n",
    "\n",
    "\n",
    "To test that the installation works, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaa60fb-a9f8-452a-83d7-943c4c85654a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda environment: explore_vcf_storage_solutions\n",
      "Current Python version: 3.13.1 | packaged by conda-forge | (main, Jan 13 2025, 09:48:16) [Clang 18.1.8 ]\n",
      "bcftools 1.21\n",
      "Using htslib 1.21\n",
      "Copyright (C) 2024 Genome Research Ltd.\n",
      "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
      "This is free software: you are free to change and redistribute it.\n",
      "There is NO WARRANTY, to the extent permitted by law.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import humanfriendly\n",
    "\n",
    "#Check that Conda and the libraries are installed as expected:\n",
    "print(f\"Current Conda environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "print(f\"Current Python version: {sys.version}\")\n",
    "!bcftools --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43673122-13d5-48ed-a5f8-e3f645366bc7",
   "metadata": {},
   "source": [
    "# 3. Preparing the example data\n",
    "\n",
    "Let's start with a small VCF dataset. A famous public dataset of genetic variants is the [1000 Genomes human VCF data](https://www.internationalgenome.org/1000-genomes-summary). Files for the lastest release (\"Phase3\") are [available here](https://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/). This data is likely too large for the simple test of functionality that we will be doing in this notebook and we will want to downsample it to speed up calculations. Nevertheless, the full version of this dataset will be useful for later investigations in other notebooks.\n",
    "\n",
    "Inspired by the [sgkit GWAS tutorial](https://github.com/sgkit-dev/sgkit/blob/7094d3cf192dfc25ff69456ec7f1e71e7df2c264/docs/examples/gwas_tutorial.ipynb), we can handle the downloaded of the VCF file the with the `requests` library if we want to keep the processing in python. (An alternative would be to use `curl`)\n",
    "\n",
    "A function that download the example data can look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ddeef2-9adc-4ffc-b1de-772ada051f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vcf(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # The response object contains binary, thus open the output file in write-binary mode\n",
    "        with open(output_file, 'wb') as file:\n",
    "            #Use chunks to speed up download. Use the default value of 8192 (2^13) bytes \n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"VCF file downloaded successfully and saved to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to download VCF file. HTTP status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42514f3-f593-428a-a118-8d6f9d9da2c8",
   "metadata": {},
   "source": [
    "For now, let's download the data for chromosome 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161cc9c0-b390-4fb3-ae17-25c033df7dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF file downloaded successfully and saved to ./input_data_temp/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
      "CPU times: user 6.85 s, sys: 5.32 s, total: 12.2 s\n",
      "Wall time: 60 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "downloaded_file = \"./input_data_temp/\" + os.path.basename(url)\n",
    "if not os.path.isfile(downloaded_file):\n",
    "    download_vcf(url, downloaded_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb61e7f-d74e-4b7b-8b47-08a5987e3483",
   "metadata": {},
   "source": [
    "Despite only spanning one single chromosome, this file is still quite large for our current purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4335b3d-8598-460a-9f67-b85917a80a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\t1.13 GiB\n"
     ]
    }
   ],
   "source": [
    "file_size = os.path.getsize(downloaded_file)\n",
    "print(f\"{os.path.basename(downloaded_file)}\\t{humanfriendly.format_size(file_size, binary=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2d53e-d63a-4968-89ea-bcaf0c545006",
   "metadata": {},
   "source": [
    "(The `binary=True` option is used to get file sizes in the same format as the `ls -lh` bash command for ease of comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99161f6-39a2-4d7a-a2b8-7b8a3a688118",
   "metadata": {},
   "source": [
    "To get a feeling for the number of variants and samples in this file, we can use the gold standard VCF parsing software `bcftools`. For speeding up access to the file, we should first index the VCF. The default settings generates a .csi (coordinate-sorted index), but it is also possible generate tabix indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44281906-a324-451a-917d-4109e86e5842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 ms, sys: 105 ms, total: 351 ms\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools index {downloaded_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321fc50-4654-4e0f-bf3a-60dd5f7bb7e0",
   "metadata": {},
   "source": [
    "Let's count the number of variants and samples. By timing the command, we can get a feeling that even with this highly optimized tool, the parsing of this file takes quite some time! The number of variants are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74403bfd-0a99-4a7e-9fb8-61fb324fdc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6468094\n",
      "CPU times: user 1.81 s, sys: 596 ms, total: 2.4 s\n",
      "Wall time: 6min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view --no-header {downloaded_file} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0b91b-9e4c-4c91-85ea-7e9cceca38f5",
   "metadata": {},
   "source": [
    "and the number of samples contained in the file are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005840d0-cb08-49d9-b4b7-5db21fdecb80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504\n",
      "CPU times: user 2.66 ms, sys: 13.7 ms, total: 16.3 ms\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools query -l {downloaded_file} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436fdef-f8d1-40a0-82b7-9aba388efff9",
   "metadata": {},
   "source": [
    "The downloaded file is a gzip compressed VCF. `bcftools` also supports the BCF (binary VCF) format. BCF is developed to for improved performance and compression. For the sake of faster operations during the preparation of the example data, we can afford to convert the file to BCF. Having access to the example data in both VCF and BCF format will also be useful for testing of ingestion operations of different VCF storage tools.\n",
    "\n",
    "Conversion from VCF to BCF is done by setting the `-O` flag to `b`(compressed BCF). For piping of `bcftools` commands, it is reccomended to use the uncompressed BCF format `-Ou` to avoid losing processing time on compression/uncompression operations, but since we are not using a pipe here, we can make the compressed BCF directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eef537f-3ea7-40fa-b52f-dc4796565e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 485 ms, total: 1.63 s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view {downloaded_file} -Ob -o ./input_data_temp/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce54121-11f8-4692-bb7e-39ba417ea645",
   "metadata": {},
   "source": [
    "Indexing of the BCF file is done with same command and will produce a `.csi` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b20033-1a7a-4226-9d27-3948af032121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: failed to open {downloaded_file_bcf}\n",
      "CPU times: user 4.4 ms, sys: 18.6 ms, total: 23 ms\n",
      "Wall time: 222 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools index {downloaded_file_bcf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4051e1-6a16-4ca5-9f4f-4d2e9d9d2c01",
   "metadata": {},
   "source": [
    "Were there any gains in file size when using the compressed BCF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74f4f4b-142a-4300-946f-e5d2d8fc24ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf\t971.47 MiB\n"
     ]
    }
   ],
   "source": [
    "downloaded_file_bcf=\"./input_data_temp/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf\"\n",
    "file_size = os.path.getsize(downloaded_file_bcf)\n",
    "print(f\"{os.path.basename(downloaded_file_bcf)}\\t{humanfriendly.format_size(file_size, binary=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5624cf-70f5-4687-9a69-7b974d2b157e",
   "metadata": {},
   "source": [
    "It is a tad bit smaller, as expected. The original `.vcf.gz` was 1.2G, for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c13dbe-7f27-4e53-9cd8-5a6e212fff1d",
   "metadata": {},
   "source": [
    "For illustration purposes, an example of an operation that is faster on the BCF is the command to count number of variants. For comparison, this took 6min 26s on the VCF.GZ version of the file. Using the BCF is faster for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd53c82a-f9d8-4df4-a0c6-17fe0bca45e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6468094\n",
      "CPU times: user 679 ms, sys: 284 ms, total: 963 ms\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view --no-header {downloaded_file_bcf} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33580af-a42b-45dc-9225-c7598c0f7893",
   "metadata": {},
   "source": [
    "For our current purposes, it is great that it is smaller, but we still want to downsample the file. First, let's make a decision to only include the first 200 samples. `bcftools query -l` produces a list of all samples in the VCF/BCF file, from which we can extract a list with the first 200 samples that can be extracted with `bcftools view -S` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3cbc583-1d78-4889-bbcc-29268208c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 108 ms, total: 231 ms\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "downloaded_file_bcf_first_200_samples = \"./input_data_temp/1kG_p3_chr1_first_200_samples.bcf\"\n",
    "!bcftools query -l {downloaded_file_bcf} > ./input_data_temp/1kG_p3_chr1_samples.txt\n",
    "!head -n 200 ./input_data_temp/1kG_p3_chr1_samples.txt > ./input_data_temp/1kG_p3_chr1_samples_first_200_samples.txt\n",
    "!bcftools view -S ./input_data_temp/1kG_p3_chr1_samples_first_200_samples.txt -Ob {downloaded_file_bcf} -o {downloaded_file_bcf_first_200_samples}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e90167-71a5-468b-8849-3e100af156f2",
   "metadata": {},
   "source": [
    "Pretty fast! (As a footnote, the time it took to perform this operation on the original .vcf.gz file on the same laptop was circa 4 minutes. The BCF format sure has performance benefits. Those results are not included here since the focus is on the downsampling, not on benchmarking `bcftools`.)\n",
    "\n",
    "Dropping these ~2000 samples gave a file size of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9ef575-6379-4f29-a9e7-35022b51d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1kG_p3_chr1_first_200_samples.bcf\t226.2 MiB\n"
     ]
    }
   ],
   "source": [
    "file_size = os.path.getsize(downloaded_file_bcf_first_200_samples)\n",
    "print(f\"{os.path.basename(downloaded_file_bcf_first_200_samples)}\\t{humanfriendly.format_size(file_size, binary=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc8e06-49b6-46eb-a7a5-3c9631c941e2",
   "metadata": {},
   "source": [
    "The downsampling is getting somewhere! \n",
    "\n",
    "However, after we now have removed more than 2000 samples from the file, there is likely to be some variants in the dataset that have no ALT alleles in our subset of 200 samples. The AC field is used to quantify this: AC=0 means that all alleles from the samples have the reference allele, and a higher value, AC>=1 means that there is >=1 non-ref alleles.\n",
    "\n",
    "To check the number variants in the 200 sample file that only contain the REF allele, we can set a filter on a maximum allowed AC with the flag `-C0`, where 0 is the max number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926ba5b6-4e8e-438c-9e6c-07833dbdd5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5141248\n",
      "CPU times: user 56.3 ms, sys: 41.2 ms, total: 97.5 ms\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view --no-header -C0 ./input_data_temp/1kG_p3_chr1_first_200_samples.bcf |wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ce3ce-559e-4c69-8f32-972ea9fc0146",
   "metadata": {},
   "source": [
    "That is quite a few! This is not that unexpected seeing that we were now acting on a subset of the first ~10% of the samples (i.e. many of the samples in the file do not have the ALT alleles). But these \"empty\" lines come with a cost for storage and parsing, so for our downsampling we can drop these lines. This time, let's set the minimum AC count to 1, using the flag `-c1` (note the lower case `c` for the min AC filter, compared to captial `C` for the max AC filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a96510a-be5b-4c83-94f4-4534afc62baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 22.8 ms, total: 45.4 ms\n",
      "Wall time: 7.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view -c1 ./input_data_temp/1kG_p3_chr1_first_200_samples.bcf -Ob -o ./input_data_temp/1kG_p3_chr1_first_200_samples_c1.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06c845-daff-4354-abb2-f328318b910a",
   "metadata": {},
   "source": [
    "This really reduced the file size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6626f5b-96fa-41dd-98c5-947dffc45638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1kG_p3_chr1_first_200_samples_c1.bcf\t72.55 MiB\n"
     ]
    }
   ],
   "source": [
    "downsampled_bcf=\"./input_data_temp/1kG_p3_chr1_first_200_samples_c1.bcf\"\n",
    "file_size = os.path.getsize(downsampled_bcf)\n",
    "print(f\"{os.path.basename(downsampled_bcf)}\\t{humanfriendly.format_size(file_size, binary=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92043c26-66f0-49ee-9490-572d274a44f3",
   "metadata": {},
   "source": [
    "Sanity-check query to ensure that there are no AC=0 variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499fe567-24a9-4157-b3cf-85002426bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: user 4.54 ms, sys: 13.7 ms, total: 18.2 ms\n",
      "Wall time: 796 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view --no-header -C0 {downsampled_bcf} |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def664e3-7355-40b7-916e-958b196d00e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326846\n",
      "CPU times: user 16.1 ms, sys: 19.7 ms, total: 35.8 ms\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view --no-header {downsampled_bcf} |wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b3a33-a43f-402a-8346-2d5cbd8a249c",
   "metadata": {},
   "source": [
    "And about 1 million variants in total. This level of downsampling is probably enough for now. Let's finish by converting the downsampled file to VCF.GZ so that we get the option to explore ingestion of BCF and VCF.GZ in the coming notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab571c81-c2a8-4a9d-85b6-aa2df97a02b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 ms, sys: 29 ms, total: 67.9 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools view {downsampled_bcf} -Oz -o ./input_data_temp/1kG_p3_chr1_first_200_samples_c1.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc203b4-a2f5-4d5c-83b5-818205fe2cbe",
   "metadata": {},
   "source": [
    "As it turns out, for this data at this degree of downsampling, there is not very big differences in file size between the BCF and VCF.GZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d4b3912-6367-494d-a07d-b90c40139ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1kG_p3_chr1_first_200_samples_c1.vcf.gz\t79.62 MiB\n",
      "1kG_p3_chr1_first_200_samples_c1.bcf\t72.55 MiB\n"
     ]
    }
   ],
   "source": [
    "downsampled_vcf_gz = \"./input_data_temp/1kG_p3_chr1_first_200_samples_c1.vcf.gz\"\n",
    "file_size = os.path.getsize(downsampled_vcf_gz)\n",
    "print(f\"{os.path.basename(downsampled_vcf_gz)}\\t{humanfriendly.format_size(file_size, binary=True)}\")\n",
    "file_size = os.path.getsize(downsampled_bcf)\n",
    "print(f\"{os.path.basename(downsampled_bcf)}\\t{humanfriendly.format_size(file_size, binary=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981126d4-066e-45f4-9ff7-4d76e57b17e4",
   "metadata": {},
   "source": [
    "Alright. With this, we now have downsampled dataset of genetic variants to use when exploring different tools for working with VCF files. We should probably still consider this a small dataset as far as population genomics is considered. But hopefully these 1 million variants and 200 samples is big enough to get processing times is seconds or minutes rather than micro- or milliseconds so that we can get a little feeling for their resource demands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
